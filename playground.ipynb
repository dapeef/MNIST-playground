{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multilayer perceptron\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(.1), # To prevent overfitting\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "model_name = \"MLP-l64-l64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define more flexible model (resnet)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(28*28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do = self.do(h2 + h1)\n",
    "        h3 = self.l3(do)\n",
    "        h4 = self.softmax(h3)\n",
    "\n",
    "        return h3\n",
    "    \n",
    "model = ResNet()\n",
    "\n",
    "model_name = \"ResNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimiser\n",
    "\n",
    "optimiser = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms to augment data\n",
    "\n",
    "class RandomPad:\n",
    "    def __init__(self, max_pad) -> None:\n",
    "        self.max_pad = max_pad\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        left = random.randint(0, self.max_pad)\n",
    "        right = random.randint(0, self.max_pad)\n",
    "        top = random.randint(0, self.max_pad)\n",
    "        bottom = random.randint(0, self.max_pad)\n",
    "\n",
    "        padding = (left, top, right, bottom)\n",
    "        # padding = (self.max_pad, self.max_pad, self.max_pad, self.max_pad)\n",
    "        return nn.functional.pad(img, padding)\n",
    "\n",
    "aug_name = \"crop-rot\"\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop((28, 28), scale=(.8, 1)),\n",
    "    RandomPad(10),\n",
    "    transforms.Resize((28, 28)),\n",
    "])\n",
    "\n",
    "# aug_name = \"no-aug\"\n",
    "# train_transforms = transforms.ToTensor()\n",
    "\n",
    "val_transforms = train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download/load MNIST dataset\n",
    "\n",
    "train_data = datasets.MNIST(\"data\", train=True, download=True)\n",
    "train, val = random_split(train_data, [55000, 5000])\n",
    "train.dataset.transform = train_transforms\n",
    "val.dataset.transform = val_transforms\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images to verify that they're ok\n",
    "\n",
    "if False:\n",
    "    counter = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        x, y = batch\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure()\n",
    "        plt.title(f\"{y[0]}\")\n",
    "        plt.imshow(x[0].squeeze(), cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if counter > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise training\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "\n",
    "exec_name = f\"{datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")}_{model_name}_{aug_name}\"\n",
    "\n",
    "writer = SummaryWriter(f\"./runs/{exec_name}\")\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "# plt.imshow(img_grid)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('batch_of_images', img_grid)\n",
    "\n",
    "# Show graph\n",
    "writer.add_graph(model, images.view(images.size(0), -1)) # flatten the images\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 || Train loss: 2.00 | Val loss: 1.72 || Train accuracy: 0.33 | Val accuracy: 0.43 ||Epoch time: 38.0979790687561\n",
      "Epoch 1 || Train loss: 1.58 | Val loss: 1.41 || Train accuracy: 0.48 | Val accuracy: 0.54 ||Epoch time: 38.19516086578369\n",
      "Epoch 2 || Train loss: 1.29 | Val loss: 1.12 || Train accuracy: 0.58 | Val accuracy: 0.65 ||Epoch time: 38.68245768547058\n",
      "Epoch 3 || Train loss: 1.05 | Val loss: 0.92 || Train accuracy: 0.67 | Val accuracy: 0.72 ||Epoch time: 38.284220457077026\n",
      "Epoch 4 || Train loss: 0.89 | Val loss: 0.80 || Train accuracy: 0.72 | Val accuracy: 0.76 ||Epoch time: 38.65808939933777\n",
      "Epoch 5 || Train loss: 0.80 | Val loss: 0.69 || Train accuracy: 0.75 | Val accuracy: 0.79 ||Epoch time: 37.93833589553833\n",
      "Epoch 6 || Train loss: 0.72 | Val loss: 0.64 || Train accuracy: 0.78 | Val accuracy: 0.81 ||Epoch time: 37.844804763793945\n",
      "Epoch 7 || Train loss: 0.66 | Val loss: 0.60 || Train accuracy: 0.80 | Val accuracy: 0.82 ||Epoch time: 37.47394561767578\n",
      "Epoch 8 || Train loss: 0.62 | Val loss: 0.54 || Train accuracy: 0.81 | Val accuracy: 0.84 ||Epoch time: 37.49191379547119\n",
      "Epoch 9 || Train loss: 0.58 | Val loss: 0.50 || Train accuracy: 0.82 | Val accuracy: 0.86 ||Epoch time: 37.566439390182495\n",
      "Epoch 10 || Train loss: 0.55 | Val loss: 0.47 || Train accuracy: 0.83 | Val accuracy: 0.86 ||Epoch time: 37.628270864486694\n",
      "Epoch 11 || Train loss: 0.52 | Val loss: 0.44 || Train accuracy: 0.84 | Val accuracy: 0.86 ||Epoch time: 37.86812353134155\n",
      "Epoch 12 || Train loss: 0.51 | Val loss: 0.44 || Train accuracy: 0.85 | Val accuracy: 0.87 ||Epoch time: 37.81788444519043\n",
      "Epoch 13 || Train loss: 0.48 | Val loss: 0.42 || Train accuracy: 0.85 | Val accuracy: 0.87 ||Epoch time: 37.63818359375\n",
      "Epoch 14 || Train loss: 0.47 | Val loss: 0.42 || Train accuracy: 0.86 | Val accuracy: 0.88 ||Epoch time: 37.667720317840576\n",
      "Epoch 15 || Train loss: 0.45 | Val loss: 0.40 || Train accuracy: 0.86 | Val accuracy: 0.88 ||Epoch time: 37.72522473335266\n",
      "Epoch 16 || Train loss: 0.44 | Val loss: 0.38 || Train accuracy: 0.86 | Val accuracy: 0.89 ||Epoch time: 39.085808753967285\n",
      "Epoch 17 || Train loss: 0.42 | Val loss: 0.37 || Train accuracy: 0.87 | Val accuracy: 0.89 ||Epoch time: 38.890918016433716\n",
      "Epoch 18 || Train loss: 0.42 | Val loss: 0.34 || Train accuracy: 0.87 | Val accuracy: 0.90 ||Epoch time: 38.915180921554565\n",
      "Epoch 19 || Train loss: 0.40 | Val loss: 0.35 || Train accuracy: 0.88 | Val accuracy: 0.90 ||Epoch time: 39.4544780254364\n",
      "Epoch 20 || Train loss: 0.40 | Val loss: 0.35 || Train accuracy: 0.88 | Val accuracy: 0.89 ||Epoch time: 39.65383291244507\n",
      "Epoch 21 || Train loss: 0.40 | Val loss: 0.35 || Train accuracy: 0.88 | Val accuracy: 0.90 ||Epoch time: 39.649282455444336\n",
      "Epoch 22 || Train loss: 0.39 | Val loss: 0.32 || Train accuracy: 0.88 | Val accuracy: 0.90 ||Epoch time: 39.64644265174866\n",
      "Epoch 23 || Train loss: 0.38 | Val loss: 0.32 || Train accuracy: 0.88 | Val accuracy: 0.90 ||Epoch time: 39.55967473983765\n",
      "Epoch 24 || Train loss: 0.38 | Val loss: 0.33 || Train accuracy: 0.89 | Val accuracy: 0.90 ||Epoch time: 39.39387011528015\n",
      "Epoch 25 || Train loss: 0.37 | Val loss: 0.31 || Train accuracy: 0.89 | Val accuracy: 0.91 ||Epoch time: 39.33300471305847\n",
      "Epoch 26 || Train loss: 0.36 | Val loss: 0.32 || Train accuracy: 0.89 | Val accuracy: 0.90 ||Epoch time: 39.3285973072052\n",
      "Epoch 27 || Train loss: 0.36 | Val loss: 0.32 || Train accuracy: 0.89 | Val accuracy: 0.90 ||Epoch time: 39.865527391433716\n",
      "Epoch 28 || Train loss: 0.36 | Val loss: 0.30 || Train accuracy: 0.89 | Val accuracy: 0.90 ||Epoch time: 39.52862524986267\n",
      "Epoch 29 || Train loss: 0.35 | Val loss: 0.30 || Train accuracy: 0.89 | Val accuracy: 0.91 ||Epoch time: 39.96325063705444\n",
      "Epoch 30 || Train loss: 0.35 | Val loss: 0.31 || Train accuracy: 0.89 | Val accuracy: 0.91 ||Epoch time: 39.510451793670654\n",
      "Epoch 31 || Train loss: 0.34 | Val loss: 0.29 || Train accuracy: 0.89 | Val accuracy: 0.91 ||Epoch time: 39.53190517425537\n",
      "Epoch 32 || Train loss: 0.34 | Val loss: 0.29 || Train accuracy: 0.90 | Val accuracy: 0.91 ||Epoch time: 39.59930372238159\n",
      "Epoch 33 || Train loss: 0.35 | Val loss: 0.29 || Train accuracy: 0.89 | Val accuracy: 0.91 ||Epoch time: 39.612200021743774\n",
      "Epoch 34 || Train loss: 0.33 | Val loss: 0.29 || Train accuracy: 0.90 | Val accuracy: 0.91 ||Epoch time: 39.53390097618103\n",
      "Epoch 35 || Train loss: 0.34 | Val loss: 0.29 || Train accuracy: 0.90 | Val accuracy: 0.91 ||Epoch time: 39.570425271987915\n",
      "Epoch 36 || Train loss: 0.33 | Val loss: 0.28 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.521655797958374\n",
      "Epoch 37 || Train loss: 0.32 | Val loss: 0.28 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.40927982330322\n",
      "Epoch 38 || Train loss: 0.32 | Val loss: 0.26 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.512407541275024\n",
      "Epoch 39 || Train loss: 0.32 | Val loss: 0.28 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.32205939292908\n",
      "Epoch 40 || Train loss: 0.32 | Val loss: 0.27 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.587159156799316\n",
      "Epoch 41 || Train loss: 0.32 | Val loss: 0.27 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.445849657058716\n",
      "Epoch 42 || Train loss: 0.31 | Val loss: 0.26 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.8476128578186\n",
      "Epoch 43 || Train loss: 0.31 | Val loss: 0.28 || Train accuracy: 0.90 | Val accuracy: 0.92 ||Epoch time: 39.90964674949646\n",
      "Epoch 44 || Train loss: 0.31 | Val loss: 0.26 || Train accuracy: 0.91 | Val accuracy: 0.92 ||Epoch time: 39.500251054763794\n",
      "Epoch 45 || Train loss: 0.31 | Val loss: 0.26 || Train accuracy: 0.91 | Val accuracy: 0.92 ||Epoch time: 39.032206773757935\n",
      "Epoch 46 || Train loss: 0.31 | Val loss: 0.26 || Train accuracy: 0.91 | Val accuracy: 0.92 ||Epoch time: 39.14554047584534\n",
      "Epoch 47 || Train loss: 0.30 | Val loss: 0.26 || Train accuracy: 0.91 | Val accuracy: 0.92 ||Epoch time: 39.29240417480469\n",
      "Epoch 48 || Train loss: 0.30 | Val loss: 0.27 || Train accuracy: 0.91 | Val accuracy: 0.91 ||Epoch time: 39.20950746536255\n",
      "Epoch 49 || Train loss: 0.30 | Val loss: 0.25 || Train accuracy: 0.91 | Val accuracy: 0.92 ||Epoch time: 39.29114651679993\n"
     ]
    }
   ],
   "source": [
    "# Train like a boss\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b * 1 * 28 * 28\n",
    "        b = x.size(0) # batch size\n",
    "        x = x.view(b, -1) # flatten the images\n",
    "\n",
    "        # execute the model forwards\n",
    "        l = model(x) # l: logits\n",
    "\n",
    "        # compute objective function (loss)\n",
    "        J = loss(l, y)\n",
    "\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Accumulate the partial derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # Step in the right direction\n",
    "        optimiser.step()\n",
    "\n",
    "        train_losses.append(J.item())\n",
    "        train_accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    model.eval() # This disables the Dropout layer\n",
    "\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b * 1 * 28 * 28\n",
    "        b = x.size(0) # batch size\n",
    "        x = x.view(b, -1) # flatten the images in the batch\n",
    "\n",
    "        # execute the model forwards\n",
    "        with torch.no_grad():\n",
    "            l = model(x) # l: logits\n",
    "\n",
    "        # compute objective function (loss)\n",
    "        J = loss(l, y)\n",
    "\n",
    "        val_losses.append(J.item())\n",
    "        val_accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())\n",
    "\n",
    "\n",
    "    train_loss = torch.tensor(train_losses).mean()\n",
    "    val_loss = torch.tensor(val_losses).mean()\n",
    "    train_accuracy = torch.tensor(train_accuracies).mean()\n",
    "    val_accuracy = torch.tensor(val_accuracies).mean()\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    print(f\"Epoch {epoch} || \" + \n",
    "          f\"Train loss: {train_loss:.2f} | Val loss: {val_loss:.2f} || \" + \n",
    "          f\"Train accuracy: {train_accuracy:.2f} | Val accuracy: {val_accuracy:.2f} ||\" +\n",
    "          f\"Epoch time: {epoch_time}\"\n",
    "    )\n",
    "    \n",
    "    writer.add_scalar(\"Training loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Validation loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Training accuracy\", train_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Epoch time\", epoch_time, epoch)\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "exec_name = \"2024-07-29T14-32-02_ResNet_crop-rot\"\n",
    "\n",
    "torch.save(model, f\"./checkpoints/{exec_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model = torch.load(\"./checkpoints/2024-07-29T14-32-02_ResNet_crop-rot.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely class is 4\n",
      "\n",
      "Prob that image is a 0: 0.00%\n",
      "Prob that image is a 1: 0.00%\n",
      "Prob that image is a 2: 0.00%\n",
      "Prob that image is a 3: 0.00%\n",
      "Prob that image is a 4: 98.51%\n",
      "Prob that image is a 5: 0.00%\n",
      "Prob that image is a 6: 0.00%\n",
      "Prob that image is a 7: 0.03%\n",
      "Prob that image is a 8: 0.00%\n",
      "Prob that image is a 9: 1.46%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate network on ./number.png\n",
    "\n",
    "# Define transformations to preprocess the image\n",
    "# Custom transformation to flatten the image\n",
    "class FlattenTransform:\n",
    "    def __call__(self, tensor):\n",
    "        return tensor.view(-1)  # Flatten the tensor\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load image\n",
    "image = Image.open(\"./number.png\")\n",
    "\n",
    "# Preprocess the image so it's the right size and shape to be fed into the model\n",
    "vis_image = transform(image)\n",
    "image = vis_image.view(-1)\n",
    "image = image.unsqueeze(0) # Add a batch dimension\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(vis_image.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Feed it into the network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result = model(image)\n",
    "\n",
    "result = nn.functional.softmax(result, dim=1)\n",
    "\n",
    "# Print result\n",
    "print(f\"Most likely class is {int(result.argmax(dim=1))}\\n\")\n",
    "\n",
    "for ind, i in enumerate(result[0]):\n",
    "    print(f\"Prob that image is a {ind}: {i*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
